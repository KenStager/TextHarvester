# NLP Corpus Builder with Content Intelligence

A comprehensive web scraping and content intelligence platform for collecting, analyzing, and structuring domain-specific text data. This versatile system helps you gather high-quality text data for training NER (Named Entity Recognition) and text classification models across any field - with a special focus on Premier League football as the demonstration domain.

## Overview

The NLP Corpus Builder is a domain-agnostic tool that enables researchers, data scientists, and AI engineers to efficiently gather and analyze high-quality text data from specific sources. The platform streamlines the entire process from source management to content extraction, classification, and knowledge extraction.

The system is designed to be memory-efficient, handling large-scale crawling jobs with thousands of pages while providing robust error handling and resource management. All content is automatically cleaned, normalized, and prepared for NER/SpanCat annotation and topic classification.

## Architecture

The system follows a modular architecture with these key components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚     â”‚         TOPIC-SPECIFIC               â”‚     â”‚                   â”‚
â”‚  NLP CORPUS     â”‚     â”‚      CONTENT INTELLIGENCE            â”‚     â”‚     CONTENT       â”‚
â”‚   BUILDER       â”‚â”€â”€â–º  â”‚              LAYER                   â”‚â”€â”€â–º  â”‚    GENERATION     â”‚
â”‚  (EXISTING)     â”‚     â”‚                                      â”‚     â”‚    FRAMEWORK      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚           KNOWLEDGE                  â”‚
                        â”‚          MANAGEMENT                  â”‚
                        â”‚            SYSTEM                    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features

### Core Web Scraper
- **Advanced source management system**
  - Organize sources into categorized lists
  - Source validation and testing
  - Predefined source lists for common domains
  - Import/export of source configurations

- **High-performance parallel web crawler**
  - Multi-threaded domain processing
  - Configurable crawl depth and link following
  - Rate limiting and robots.txt compliance
  - User agent rotation and request throttling
  - Robust error handling and retry mechanisms

- **Intelligent content extraction**
  - Precise main content detection with trafilatura
  - Rust-based high-performance extraction (optional)
  - Fallback extraction mechanisms
  - Advanced text cleaning and normalization
  - Metadata extraction and storage

- **Comprehensive job management**
  - Real-time job monitoring and analytics
  - Pausable/stoppable background jobs
  - Detailed logging and diagnostics
  - Job comparison and trend analysis

- **Memory-efficient data export**
  - Streaming export for large datasets
  - Chunking algorithms for NER/SpanCat training
  - Customizable chunk sizes and overlaps
  - Export with or without original HTML
  - JSONL format compatible with annotation tools

### Content Intelligence Layer

- **Topic Classification System** (Implemented)
  - Hierarchical topic taxonomy management
  - Fast filtering for rapid content relevance screening
  - Multiple classifier implementations (SVM, LR, RF)
  - Football-specific taxonomy with detailed categories
  - Prodigy integration for annotation and training

- **Named Entity Recognition** (Implemented)
  - Custom entity type definitions with inheritance
  - Specialized football entity taxonomy
  - Domain-specific entity recognition models
  - Entity linking to knowledge base
  - Relationship extraction between entities

- **Knowledge Management System** (In Development)
  - Knowledge graph representation
  - Entity and relationship storage
  - Contradiction detection
  - Source credibility assessment
  - Querying and visualization tools

## Current Implementation Status

- âœ… **Core Web Scraper**: Fully implemented with high-performance extraction
- âœ… **Database Foundation**: Schema and ORM models implemented
- âœ… **Core Framework**: Configuration, utilities, and base pipeline
- âœ… **Topic Classification**: Complete football-specific implementation
- âœ… **Entity Recognition**: Complete football-specific implementation
- ğŸ”„ **Rust Integration**: Optional high-speed content extraction engine
- ğŸš§ **Knowledge Management**: In development
- ğŸ”œ **Content Generation**: Planned for future development

## Project Structure

```
TextHarvester/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ run_text_harvester.py
â”œâ”€â”€ TextHarvester/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ data/         # Database and local storage
â”‚   â”œâ”€â”€ static/       # Web interface assets
â”‚   â”œâ”€â”€ templates/    # HTML templates
â”‚   â”œâ”€â”€ scraper/      # Web scraper components
â”‚   â”‚   â”œâ”€â”€ content_extractor.py
â”‚   â”‚   â”œâ”€â”€ crawler.py
â”‚   â”‚   â”œâ”€â”€ export.py
â”‚   â”‚   â”œâ”€â”€ rust_integration.py
â”‚   â”‚   â”œâ”€â”€ source_lists.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ rust_extractor/  # Rust-based extractor (optional)
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ api/          # API endpoints
â”‚       â”œâ”€â”€ routes.py
â”‚       â””â”€â”€ sources.py
â”œâ”€â”€ intelligence/     # Content intelligence components
â”‚   â”œâ”€â”€ base_pipeline.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ classification/  # Topic classification
â”‚   â”‚   â”œâ”€â”€ classifiers.py
â”‚   â”‚   â”œâ”€â”€ fast_filter.py
â”‚   â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”‚   â”œâ”€â”€ topic_taxonomy.py
â”‚   â”‚   â””â”€â”€ taxonomies/
â”‚   â”‚       â””â”€â”€ football.py
â”‚   â”œâ”€â”€ entities/     # Entity recognition
â”‚   â”‚   â”œâ”€â”€ entity_types.py
â”‚   â”‚   â”œâ”€â”€ linking.py
â”‚   â”‚   â”œâ”€â”€ ner_model.py
â”‚   â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”‚   â””â”€â”€ taxonomies/
â”‚   â”‚       â””â”€â”€ football_entities.py
â”‚   â”œâ”€â”€ knowledge/    # Knowledge management (in development)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ utils/        # Utility functions
â”‚       â”œâ”€â”€ model_utils.py
â”‚       â”œâ”€â”€ prodigy_integration.py
â”‚       â””â”€â”€ text_processing.py
â”œâ”€â”€ db/              # Database models and migrations
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ topic_taxonomy.py
â”‚       â”œâ”€â”€ entity_models.py
â”‚       â””â”€â”€ content_intelligence.py
â”œâ”€â”€ prodigy/         # Prodigy annotation recipes
â”‚   â””â”€â”€ recipes/
â”‚       â”œâ”€â”€ domain_ner.py
â”‚       â””â”€â”€ topic_classification.py
â””â”€â”€ tests/           # Test cases
    â””â”€â”€ integration/
        â””â”€â”€ test_rust_classification.py
```

## Installation

### Prerequisites

- Python 3.9+
- PostgreSQL database (optional, SQLite works for development)
- Rust (optional, for high-performance extraction)

### Quick Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd TextHarvester
```

2. Run the setup script:
```bash
python setup.py
```

3. Run the system:
```bash
python run_text_harvester.py
```

4. Access the web interface:
Open your browser and go to http://localhost:5000

### Manual Setup

1. Set up Python virtual environment (recommended):
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure the database:
- Copy `.env.example` to `.env` in the TextHarvester directory
- Edit `.env` to configure database connection (SQLite is used by default)

4. Run the application:
```bash
python run_text_harvester.py
```

## Usage

### Web Interface

The web interface provides comprehensive management of all scraping and intelligence functions:

1. **Source Management**:
   - Create and organize sources into lists
   - Test source extraction quality
   - Import/export source configurations

2. **Scraping Jobs**:
   - Configure job parameters (depth, rate limits, etc.)
   - Monitor job progress in real-time
   - View performance metrics and statistics
   - Export results in various formats

3. **Content Intelligence**:
   - View topic classification for extracted content
   - Explore entity recognition results
   - Browse knowledge graph (when implemented)

### API Usage

The system provides a RESTful API for integration with other applications:

```python
import requests

# Start a new scraping job
job_config = {
    "source_list_id": 1,
    "max_depth": 2,
    "follow_links": True,
    "rate_limit": 5  # requests per second
}
response = requests.post("http://localhost:5000/api/jobs", json=job_config)
job_id = response.json()["job_id"]

# Get job status
job_status = requests.get(f"http://localhost:5000/api/jobs/{job_id}").json()

# Export job results
export_config = {
    "job_id": job_id,
    "format": "jsonl",
    "include_html": False,
    "chunking": {
        "enabled": True,
        "size": 500,
        "overlap": 50
    }
}
export_url = requests.post("http://localhost:5000/api/export", json=export_config).json()["url"]
```

## Football Domain Implementation

As a demonstration domain, the system includes specialized components for Premier League football:

### Topic Classification Taxonomy
- Complete Premier League team hierarchy (all 20 teams)
- Player categories (goalkeepers, defenders, midfielders, forwards)
- Matches, transfers, competitions
- Statistics, venues, finances, media coverage

### Entity Recognition
- Team detection with nickname handling
- Player identification
- Match event extraction
- Competition and venue recognition

## Customization

The system is designed to be easily adaptable to new domains:

1. **Create a new taxonomy**:
```python
from intelligence.classification.topic_taxonomy import TopicTaxonomy, TopicNode

# Create root taxonomy
my_taxonomy = TopicTaxonomy(
    name="my_domain",
    description="My custom domain taxonomy"
)

# Create main category
main_category = TopicNode(
    name="Main Category", 
    keywords=["keyword1", "keyword2"]
)
my_taxonomy.add_root_node(main_category)

# Add subcategories
subcategory = TopicNode(
    name="Subcategory",
    keywords=["specific1", "specific2"]
)
main_category.add_child(subcategory)

# Save to database
my_taxonomy.save_to_database()
```

2. **Create domain-specific entity types**:
```python
from intelligence.entities.entity_types import EntityType

# Create custom entity types
DOMAIN_ENTITY_TYPES = {
    "CUSTOM_ENTITY": {
        "subtypes": ["SUBTYPE1", "SUBTYPE2"],
        "attributes": ["attribute1", "attribute2"]
    }
}
```

3. **Implement custom pipelines**:
```python
from intelligence.classification.pipeline import ClassificationPipeline
from intelligence.entities.pipeline import EntityExtractionPipeline

# Create custom pipelines
my_classification_pipeline = ClassificationPipeline(
    taxonomy=my_taxonomy,
    confidence_threshold=0.6,
    domain_name="my_domain"
)

my_entity_pipeline = EntityExtractionPipeline(
    entity_types=DOMAIN_ENTITY_TYPES,
    domain_name="my_domain"
)
```

## Requirements

- Python 3.9+
- PostgreSQL database (optional, SQLite works for development)
- Required Python packages (installed by setup.py):
  - flask & flask-sqlalchemy: Web framework and ORM
  - beautifulsoup4: HTML parsing
  - trafilatura: Main content extraction
  - scikit-learn: Machine learning models
  - requests: HTTP client
  - python-dotenv: Environment variable management

## Optional Components

- **Rust Extractor**: High-performance content extraction (requires Rust installation)
- **Prodigy Integration**: For training custom models (requires Prodigy license)
- **PostgreSQL**: For production deployment (SQLite works for development)

## Common Use Cases

- **Academic Research**: Collect domain-specific corpora for research in linguistics, NLP, or domain-specific studies
- **AI Training Data**: Build custom datasets for training specialized language models or NER systems
- **Market Intelligence**: Monitor specific sources for competitive intelligence and trend analysis
- **Knowledge Base Construction**: Gather domain knowledge for expert systems or semantic databases
- **Educational Content**: Collect learning materials on specific topics for educational applications
- **Media Monitoring**: Track publications across multiple sources for specific topics or entities

## Troubleshooting

### Database Issues
- Check that the data directory exists and has proper permissions
- For SQLite: Delete any corrupted database file
- For PostgreSQL: Verify connection parameters in .env file

### Rust Extractor Issues
- Ensure Rust and Cargo are installed
- Check that the Rust extractor binary is properly built
- Set USE_PYTHON_EXTRACTION=1 in .env to force Python extraction

### Web Interface Issues
- Clear browser cache and cookies
- Check for JavaScript console errors
- Verify that all static assets are loading

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Trafilatura for content extraction
- Flask for web framework
- scikit-learn for machine learning models
- Prodigy for annotation integration
- Rust for high-performance content extraction (optional component)
